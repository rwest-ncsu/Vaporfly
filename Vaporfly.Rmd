---
title: "Vaporfly"
author: "Robert West"
date: "5/11/2021"
output: 
  github_document:
    toc: TRUE
    pandoc_args: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F)
```

# Introduction

Since their introduction to the market in 2016, Nike’s `Vaporfly` shoes have set the marathon community ablaze. The shoes are supposedly designed with the intent of optimizing every step that a runner takes. Huge accomplishments in marathon racing have been made during this time as well, including Nike’s project of breaking the daunting 2-hour barrier. The release of `Vaporflys` along with the decrease in marathon times has led many to criticize the use of the shoes, suggesting that there is a substantial mechanical advantage to those who wear them. Some have gone as far to call the use of the shoes “Mechanical Doping” and call for bans on them in official races. The goal of this analysis is not to argue for or against their use, but simply to determine if the use of Nike Vaporflys has any effect on the time of marathon runners and if that effect is changed across gender, individual runner, and marathon course. 

# Packages 

```{r}
library(rjags)#MCMC
library(coda)#MCMC
library(ggplot2)#Data Visualization
library(tidyr)#Data tidying
library(dplyr)#Data wrangling
library(varhandle)
library(knitr)#Display tables
library(readr)
```

# Data Import

```{r}
men = read_csv("men_sampled_shoe.csv", col_names=T)%>%
  select(name_age, match_name, marathon, year, time_minutes, vaporfly)%>%
  mutate(sex=0)
women = read_csv("women_sampled_shoe.csv", col_names=T)%>%
  select(name_age, match_name, marathon, year, time_minutes, vaporfly)%>%
  mutate(sex=1)
```


# Data Cleaning
```{r}
final = full_join(men, women) %>%
  mutate(
    name_age = trimws(name_age, which="right"),
    age = ifelse(check.numeric(substr(name_age, nchar(name_age)-2, nchar(name_age)-1)), 
                 as.numeric(substr(name_age, nchar(name_age)-2, nchar(name_age)-1)),
                 0)
  )%>%
  filter(!is.na(vaporfly))
```


# Visualizations
```{r, echo=F}
ggplot(data=final, mapping=aes(x=time_minutes))+
  labs(x="Time in Minutes", title = "FIGURE 1: Histogram of Time(min) for all runners")+
  geom_histogram(bins=75, fill="red", col="black")

ggplot(data=final%>%filter(sex==F), mapping=aes(x=log(time_minutes)))+
  labs(title="FIGURE 2: Distribution of Men's log time", x="Men's Log minutes")+
  geom_histogram(bins=85, fill="blue", col="black")

ggplot(data=final%>%filter(sex==T), mapping=aes(x=log(time_minutes)))+
  labs(title="FIGURE 3: Distribution of Women's log time", x="Women's Log minutes")+
  geom_histogram(bins=85, fill="pink", col="black")

ggplot(data=final, mapping=aes(x=log(time_minutes)), fill=as.factor(sex))+
  labs(title="FIGURE 2: Histogram of log(time)", x="log(Time)")+
  geom_histogram(bins=75, col="black")

ggplot(data=final, mapping=aes(x=time_minutes, fill=vaporfly))+
  geom_histogram(bins = 60)

ggplot(data=final%>%filter(sex==0), mapping=aes(x=time_minutes, fill=vaporfly))+
  geom_histogram(bins = 60)

ggplot(data=final%>%filter(sex==1), mapping=aes(x=time_minutes, fill=vaporfly))+
  geom_histogram(bins = 60)
```

# Methods

To determine the effect of Vaporflys, I utilized a mixture of Bayesian Linear Regression, Bayesian Mixed-Random Effects, and Bayesian Multi-level Interaction Models to attempt to explain the overall effect that the shoes have on marathon times when applied to different genders, runners, and courses. Upon first inspection, the distribution of time was bi-modal (figure 1). 

```{r, echo=F}
ggplot(data=final, mapping=aes(x=time_minutes))+
  labs(x="Time in Minutes", title = "FIGURE 1: Histogram of Time(min) for all runners")+
  geom_histogram(bins=75, fill="brown3", col="black")
```

The 2 peaks of this distribution were explained by the differences in male and female performances. The distributions of time conditioned on sex were both bell-shaped and skewed to the right, which suggests that sex should play a role in the marathon time of an individual. While not surprising, this added the need to include sex as a predictor in each model to avoid fitting every model for men and women.

```{r, echo=F}
ggplot(data=final, mapping=aes(x=time_minutes))+
  labs(x="Time in Minutes", 
       title = "Histogram of Time(min) for all runners by Sex", 
       fill="Sex")+
  geom_histogram(mapping=aes(fill=factor(sex, levels = c(0, 1), labels=c("Male", "Female"))), bins=75, alpha=0.7, position = "identity")
```

For every model, instead of treating the raw time as the response, I treat log(time) as the response as the time variable was a little too right-skewed to meet the Normality condition of my models. Taking the log allowed for more reasonable Gaussian assumptions (Figure 2, 3). Using a consistent Log-Normal likelihood allowed for more precise model comparison after fitting as well.

```{r, echo=F}
ggplot(data=final%>%filter(sex==F), mapping=aes(x=log(time_minutes)))+
  labs(title="FIGURE 2: Distribution of Men's log time", x="Men's Log minutes")+
  geom_histogram(bins=85, fill="lightblue2", col="black")

ggplot(data=final%>%filter(sex==T), mapping=aes(x=log(time_minutes)))+
  labs(title="FIGURE 3: Distribution of Women's log time", x="Women's Log minutes")+
  geom_histogram(bins=85, fill="pink", col="black")
```

# Creating analysis variables

```{r}
y = final$time_minutes
logy = log(y)
X = cbind(1, final)
n = length(logy)
```


# Models Under Consideration

## Model 1

$$
log(Y_i) \sim N(\mu_i, \sigma^2)\\
\mu_i = B_0 + B_1V_i\\
V_i \in {0,1}: Vaporfly_i\\
B_0, B_1 \sim N(0, (\sqrt{10})^2)\\
\sigma^2 \sim InvGamma(0.1, 1)
$$

This model suggests that the only contributing factor to a runner's time is the binary `vaporfly` variable: 

```{r}
X_mod = X%>%select(1, vaporfly)%>%
  mutate(vaporfly = as.numeric(vaporfly))
data = list(X_mod=X_mod, n=n, logy=logy)

model_string = textConnection("model{
    #Likelihood 
    for(i in 1:n){
      logy[i] ~ dnorm(mu[i], tau)
      mu[i] = B1*X_mod[i,1] + B2*X_mod[i,2]
    }
    
    #Priors
    B1 ~ dnorm(0, 1/10)
    B2 ~ dnorm(0, 1/10)
    tau ~ dgamma(0.1, 1)
    sigma = 1/sqrt(tau)
}")

#Initialize parameters and construct model
inits = list(B1=0, B2=0, tau=1)
model = jags.model(model_string, data=data, inits=inits, n.chains=2, quiet=T)

#Thin the burn-in samples
update(model, 1000, progress.bar="none")

params = c("B1","B2","sigma")
samples = coda.samples(model, variable.names = params, n.iter=2000, progress.bar="none")

#Compute DIC
dic_1 = dic.samples(model, n.iter=2000, progress.bar="none")
```

```{r, echo=F}
summary(samples)
plot(samples)
gelman.diag(samples)
```

By popular convergence diagnostics (Gelman Statistic and Trace Plots), this model has converged to the parameter estimates given. Now, we fit more sophisticated models to attempt to characterize the relationship more thoroughly.

## Model 2

$$
log(Y_i) \sim N(\mu_i, \sigma^2)\\
\mu_i = B_0+B_1V_i+B_2S_i\\
V_i \in 0,1: Vaporfly\\
S_i \in 0,1: Sex\\
B_0, B_1, B_2 \sim N(0, (\sqrt{10})^2)\\
\sigma^2 \sim InvGamma(0.1, 1)\\
$$

```{r}
X_mod = X%>%select(1, vaporfly, sex)%>%
  mutate(vaporfly = as.numeric(vaporfly))
p = ncol(X_mod)
data = list(X_mod = X_mod, logy=logy, n=n, p=p)

model_string = textConnection("model{
    #Likelihood
    for(i in 1:n){
      logy[i] ~ dnorm(mu[i], tau)
      mu[i] = inprod(X_mod[i,], B[])
    }
    
    #Priors
    for(j in 1:p){
      B[j] ~ dnorm(0, 1/10)
    }
    
    tau ~ dgamma(0.1, 0.1)
    sigma = 1/sqrt(tau)
}")

inits = list(B=rep(0,p), tau=1)
model = jags.model(model_string, data=data, inits=inits, n.chains=2, quiet=T)

update(model, 10000, progress.bar="none")

params = c("B","sigma")
samples = coda.samples(model, variable.names = params, n.iter=3000, progress.bar="none")

#Convergence criterion
summary(samples)
gelman.diag(samples)

#Compute DIC
dic_2 = dic.samples(model, n.iter=3000, progress.bar="none")
```

## Model 3

$$
log(Y_i) \sim N(\mu_i, \sigma^2)\\
\mu_i = \beta_0+\beta1V_i+\beta_2S_i+\beta_3S_i*V_i\\
V_i \in 0,1: Vaporfly\\
S_i \in 0,1: Sex\\
\beta_i \sim N(0, 10^2)\\
\sigma^2 \sim InvGamma(0.1, 1)\\
$$

```{r}
X_mod = X%>%select(1, vaporfly, sex)%>%
  mutate(vaporfly = as.numeric(vaporfly))
p = ncol(X_mod)

data = list(logy=logy, X_mod=X_mod, p=p, n=n)

model_string = textConnection("model{
    #Likelihood
    for(i in 1:n){
      logy[i] ~ dnorm(mu[i], tau)
      mu[i] = inprod(X_mod[i,], B[]) + C*X_mod[i,2]*X_mod[i,3]
    }

    #Priors
    for(j in 1:p){
      B[j] ~ dnorm(0, 0.01)
    }
    
    C ~ dnorm(0, 0.01)
    tau ~ dgamma(0.1, 1)
    sigma = 1/sqrt(tau)
}")

inits = list(B=rep(0, p), C=0, tau=1)
model = jags.model(model_string, data=data, inits=inits, n.chains=2, quiet=T)
update(model, 1000, progress.bar="none")

params = c("B", "C", "sigma")
samples = coda.samples(model, n.iter=2000, progress.bar="none", variable.names = params)

summary(samples)
gelman.diag(samples)

dic_3 = dic.samples(model, n.iter=2000, progress.bar="none")
```


## Model 4

$$
log(Y_i) \sim N(\mu_i, \sigma^2)\\
\mu_i = \beta_0+\beta1V_i+\beta_2S_i+\beta_3S_i*V_i + \alpha_i\\
V_i \in 0,1: Vaporfly\\
S_i \in 0,1: Sex\\
\alpha_i: marathon \space effect\\
\beta_i \sim N(0, 10^2)\\
\alpha_i \sim N(0, 10^2)
\sigma^2 \sim InvGamma(0.1, 1)\\
$$

```{r}
X_mod = X%>%select(1, vaporfly, sex, marathon)%>%
  mutate(vaporfly = as.numeric(vaporfly),
         marathon = as.numeric(factor(marathon)))
n_mar = length(unique(X_mod$marathon))

data = list(logy=logy, n_mar=n_mar, X_mod=X_mod, n=n)

model_string = textConnection("model{
  #Likelihood
  for(i in 1:n){
    logy[i] ~ dnorm(mu[i], tau)
    mu[i] = B0 + B1*X_mod[i,2] +B2*X_mod[i,3] + alpha[X_mod[i,4]]
  }
  
  #Random effects
  for(j in 1:n_mar){
    alpha[j] ~ dnorm(0, 10)
  }
  
  B0 ~ dnorm(0, 10)
  B1 ~ dnorm(0, 10)
  B2 ~ dnorm(0, 10)
  tau ~ dgamma(0.1, 1)
  sigma = 1/sqrt(tau)
}")

inits_random = list(B0=0, B1=0, B2=0, alpha=rep(0, n_mar), tau=1)
model = jags.model(model_string, data=data, inits = inits_random, n.chains = 2, quiet=T)
update(model, 10000, progress.bar="none")

params = c("B0", "B1", "B2", "alpha", "sigma")
samples = coda.samples(model, variable.names = params, n.iter=50000, progress.bar="none")

gelman.diag(samples)

#Compute DIC
dic_4 = dic.samples(model, n.iter=50000, progress.bar="none")
```

This model suggests that there is no significant difference between marathon courses since the effective sample sizes are so small even after 50,000 iterations of MCMC. From this, I conclude that the marathon effects are constant and are "encoded" in the constant intercept $\beta_0$. For further justification: 

```{r}
alphas = apply(samples[[1]][ , 4:26],
      MARGIN = 2,
      FUN = function(x){
        c(mean(x), sd(x))
      })

alphas %>%
  t()%>%
  as.data.frame()%>%
  rename(mean=V1, sd=V2)%>%
  ggplot(mapping=aes(x=1:23, y=mean))+
  geom_point(aes(color="Mean"))+
  geom_point(aes(y=sd, color="sd"))+
  labs(color="Measurement", x=expression(alpha), 
       y="Value", main="Mean and Standard Deviation of Alpha effects")
```

Clearly, all of the $\alpha$ effects behave similarly and produced relatively similar estimates and Posterior Distributions. Because of this and the low effective sample size of the $\beta_0$ in this model, I propose the removal of the Marathon effect term in the model. However, we have yet to consider the interaction that the use of Vaporflys may have with age and marathon course. Good modeling practice says to include all lower order terms, so I will keep the marathon effect. 

## Model 5

$$
log(Y_i) \sim N(\mu_i, \sigma^2)\\
\mu_i = B_0+B_1V_i+B_2S_i+M_i+R_i+C_1M_iR_i+C_2R_iV_i+C_3S_iV_i+C_4M_iV_i\\
V_i : Indicator \space of \space Vaporflys\\
S_i : Sex\\
M_i: Effect \space of \space Marathon \space of \space Y_i\\
R_i: Effect \space of \space Runner \space of \space Y_i\\
B_0, B_1, B_2 \sim N(0, (1/10)^2)\\
M_i \sim N(0, (1/10)^2)\\
R_i \sim N(0, (1/10)^2)\\
C_1, C_2, C_3, C_4 \sim N(0, (1/10)^2)\\
\sigma^2 \sim InvGamma(0.1, 1)
$$



```{r}
X_mod = X%>%select(1, vaporfly, sex, marathon, match_name)%>%
  mutate(marathon = as.numeric(factor(marathon)),
         vaporfly = as.numeric(vaporfly),
         runner = as.numeric(factor(match_name)))%>%
  select(-match_name)

n_run = max(X_mod$runner)
n_mar = max(X_mod$marathon)

data = list(logy=logy, X_mod=X_mod, n_run=n_run, n_mar=n_mar, n=n)

model_string = textConnection("model{
    #likelihood
    for(i in 1:n){
      logy[i] ~ dnorm(mu[i], tau)
      mu[i] = B0 + B1*X_mod[i,2] + B2*X_mod[i,3] + 
      M[X_mod[i,4]] + R[X_mod[i,5]] + 
      C1*M[X_mod[i,4]]*R[X_mod[i,5]] + C2*R[X_mod[i,5]]*X_mod[i,2] + 
      C3*X_mod[i,3]*X_mod[i,2] + C4*X_mod[i,2]*M[X_mod[i,4]]
    }
    
    #Random marathon effect
    for(j in 1:n_mar){
      M[j] ~ dnorm(0, 1)
    }
    
    #Random runner effect
    for(k in 1:n_run){
      R[k] ~ dnorm(0, 1)
    }
    
    #Priors
    B0~dnorm(0, 1)
    B1~dnorm(0, 1)
    B2~dnorm(0, 1)
    C1~dnorm(0, 1)
    C2~dnorm(0, 1)
    C3~dnorm(0, 1)
    C4~dnorm(0, 1)
    tau~dgamma(0.1, 1)
    sigma = 1/sqrt(tau)
}")

inits = list(B0=0, B1=0, B2=0, C1=0, C2=0, C3=0,C4=0, M=rep(0, n_mar), R=rep(0, n_run), tau=1)
model = jags.model(model_string, data=data, inits = inits, n.chains = 2, quiet=T)
update(model, 10000, progress.bar="none")

params = c("B0", "B1", "B2","C1", "C2", "C3", "C4", "sigma", "M", "R")
samples = coda.samples(model, variable.names = params, n.iter=10000, progress.bar="none")

dic_5 = dic.samples(model, n.iter=1000, progress.bar="none")
```

  






